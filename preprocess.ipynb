{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess the data\n",
    "\n",
    "This script preprocesses data from Stack Overflow Developer Survey.\n",
    "\n",
    "- download the data from https://survey.stackoverflow.co/ if not already downloaded\n",
    "- extract relevant columns, convert them to uniform format\n",
    "- clean data: remove outliers by salary, years of experience, age, etc..\n",
    "- save into `src/assets/data.csv` from where the frontend will load it\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping 2017 data download, already exists\n",
      "Skipping 2018 data download, already exists\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/y5/_fnn1tf53cz1df_1wxhgm5m00000gn/T/ipykernel_44905/1358424381.py:54: DtypeWarning: Columns (8,12,13,14,15,16,50,51,52,53,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  dfs_by_year[year] = pd.read_csv(csv_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping 2019 data download, already exists\n",
      "Skipping 2020 data download, already exists\n",
      "Skipping 2021 data download, already exists\n",
      "Skipping 2022 data download, already exists\n",
      "Skipping 2023 data download, already exists\n",
      "Skipping 2024 data download, already exists\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import zipfile\n",
    "import requests\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "# URLs for the .zip files by year\n",
    "urls_by_year = {\n",
    "    \"2017\": \"https://survey.stackoverflow.co/datasets/stack-overflow-developer-survey-2017.zip\",\n",
    "    \"2018\": \"https://info.stackoverflowsolutions.com/rs/719-EMH-566/images/stack-overflow-developer-survey-2018.zip\",\n",
    "    \"2019\": \"https://info.stackoverflowsolutions.com/rs/719-EMH-566/images/stack-overflow-developer-survey-2019.zip\",\n",
    "    \"2020\": \"https://survey.stackoverflow.co/datasets/stack-overflow-developer-survey-2020.zip\",\n",
    "    \"2021\": \"https://info.stackoverflowsolutions.com/rs/719-EMH-566/images/stack-overflow-developer-survey-2021.zip\",\n",
    "    \"2022\": \"https://info.stackoverflowsolutions.com/rs/719-EMH-566/images/stack-overflow-developer-survey-2022.zip\",\n",
    "    \"2023\": \"https://cdn.stackoverflow.co/files/jo7n4k8s/production/49915bfd46d0902c3564fd9a06b509d08a20488c.zip\",\n",
    "    \"2024\": \"https://cdn.sanity.io/files/jo7n4k8s/production/262f04c41d99fea692e0125c342e446782233fe4.zip\",\n",
    "}\n",
    "\n",
    "# Directory to store raw data\n",
    "raw_data_dir = Path(\"data\") / \"raw\"\n",
    "\n",
    "# Ensure the directory exists\n",
    "raw_data_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def download_and_extract(year: str, url: str) -> Path:\n",
    "    save_path = raw_data_dir / f\"survey-{year}\"\n",
    "    if save_path.exists():\n",
    "        # Already downloaded\n",
    "        print(f\"Skipping {year} data download, already exists\")\n",
    "        return save_path / \"survey_results_public.csv\"\n",
    "\n",
    "    # Create the zip file path\n",
    "    zip_file_path = raw_data_dir / f\"stack-overflow-developer-survey-{year}.zip\"\n",
    "\n",
    "    # Download the .zip file\n",
    "    print(f\"Downloading {year} data...\")\n",
    "    response = requests.get(url)\n",
    "    with open(zip_file_path, \"wb\") as file:\n",
    "        file.write(response.content)\n",
    "\n",
    "    with zipfile.ZipFile(zip_file_path, \"r\") as zip_ref:\n",
    "        zip_ref.extractall(save_path)\n",
    "\n",
    "    # Clean up the zip file after extraction\n",
    "    os.remove(zip_file_path)\n",
    "\n",
    "    # Return the path to the CSV file inside the extracted folder\n",
    "    return save_path / \"survey_results_public.csv\"\n",
    "\n",
    "# Download and extract the data, then read it into pandas DataFrame\n",
    "dfs_by_year = {}\n",
    "for year, url in urls_by_year.items():\n",
    "    csv_path = download_and_extract(year, url)\n",
    "    dfs_by_year[year] = pd.read_csv(csv_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## unify the datasets\n",
    "\n",
    "- rename columns to uniform names\n",
    "- convert years/age ranges (e.g. \"20 to 30 years old\") to values (using interval midpoints)\n",
    "- remove rows that have missing values for salary, programming langs, country or years of experience\n",
    "- approximate age using years of experience if the column is missing (in the 2017 survey)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "\n",
    "# These columns are used in the frontend\n",
    "class Column(Enum):\n",
    "    SALARY = \"salary\"\n",
    "    COUNTRY = \"country\"\n",
    "    COUNTRY_CODE = \"country_code\"\n",
    "    YEARS_OF_EXPERIENCE = \"years_of_experience\"\n",
    "    AGE = \"age\"\n",
    "    PROGRAMMING_LANGUAGE = \"language\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_map_2021_and_later = {\n",
    "    Column.SALARY: \"ConvertedCompYearly\",\n",
    "    Column.COUNTRY: \"Country\",\n",
    "    Column.COUNTRY_CODE: None,\n",
    "    Column.YEARS_OF_EXPERIENCE: \"YearsCode\",\n",
    "    Column.AGE: \"Age\",\n",
    "    Column.PROGRAMMING_LANGUAGE: \"LanguageHaveWorkedWith\",\n",
    "}\n",
    "\n",
    "colum_map_by_year = {\n",
    "    \"2017\": {\n",
    "        Column.SALARY: \"Salary\",\n",
    "        Column.COUNTRY: \"Country\",\n",
    "        Column.COUNTRY_CODE: None,\n",
    "        Column.YEARS_OF_EXPERIENCE: \"YearsProgram\",\n",
    "        Column.AGE: None,  # Approximated YearsProgram + 23\n",
    "        Column.PROGRAMMING_LANGUAGE: \"HaveWorkedLanguage\"\n",
    "    },\n",
    "    \"2018\": {\n",
    "        Column.SALARY: \"ConvertedSalary\",\n",
    "        Column.COUNTRY: \"Country\",\n",
    "        Column.COUNTRY_CODE: None,\n",
    "        Column.YEARS_OF_EXPERIENCE: \"YearsCoding\",\n",
    "        Column.AGE: \"Age\",\n",
    "        Column.PROGRAMMING_LANGUAGE: \"LanguageWorkedWith\",\n",
    "    },\n",
    "    \"2019\": {\n",
    "        Column.SALARY: \"ConvertedComp\",\n",
    "        Column.COUNTRY: \"Country\",\n",
    "        Column.COUNTRY_CODE: None,\n",
    "        Column.YEARS_OF_EXPERIENCE: \"YearsCode\",\n",
    "        Column.AGE: \"Age\",\n",
    "        Column.PROGRAMMING_LANGUAGE: \"LanguageWorkedWith\",\n",
    "    },\n",
    "    \"2020\": {\n",
    "        Column.SALARY: \"ConvertedComp\",\n",
    "        Column.COUNTRY: \"Country\",\n",
    "        Column.COUNTRY_CODE: None,\n",
    "        Column.YEARS_OF_EXPERIENCE: \"YearsCode\",\n",
    "        Column.AGE: \"Age\",\n",
    "        Column.PROGRAMMING_LANGUAGE: \"LanguageWorkedWith\",\n",
    "    },\n",
    "    \"2021\": column_map_2021_and_later,\n",
    "    \"2022\": column_map_2021_and_later,\n",
    "    \"2023\": column_map_2021_and_later,\n",
    "    \"2024\": column_map_2021_and_later,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert years/age ranges to values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def convert_years_and_age(df: pd.DataFrame):\n",
    "    def convert_to_numeric(value):\n",
    "        if pd.isnull(value):\n",
    "            return None\n",
    "        if isinstance(value, (int, float)):  # If already a number, return it\n",
    "            return float(value)\n",
    "        value = str(value)\n",
    "\n",
    "        # Match patterns and convert accordingly\n",
    "        if re.match(r'^\\d+$', value):\n",
    "            return float(value)\n",
    "        elif match := re.match(r'^(\\d+)\\s*years?', value):\n",
    "            return float(match.group(1))\n",
    "        elif match := re.match(r'^(\\d+)\\s*to\\s*(\\d+)', value):\n",
    "            return (float(match.group(1)) + float(match.group(2))) / 2\n",
    "        elif match := re.match(r'^(\\d+)[\\s\\-]+(\\d+)', value):\n",
    "            return (float(match.group(1)) + float(match.group(2))) / 2\n",
    "        elif match := re.match(r'^(\\d+)\\s*and\\s*more', value):\n",
    "            return float(match.group(1)) + 1\n",
    "        elif match := re.match(r'^(\\d+)\\s*or\\s*more', value):\n",
    "            return float(match.group(1)) + 1\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    # Apply conversion to specified columns\n",
    "    df[Column.YEARS_OF_EXPERIENCE.value] = df[Column.YEARS_OF_EXPERIENCE.value].apply(convert_to_numeric)\n",
    "    df[Column.AGE.value] = df[Column.AGE.value].apply(convert_to_numeric)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract uniform country codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pycountry\n",
    "import country_converter as coco\n",
    "import logging\n",
    "\n",
    "country_converter = coco.CountryConverter(include_obsolete=True)\n",
    "\n",
    "# Do not log warnings from coco (if match is not found)\n",
    "logging.basicConfig(level=logging.ERROR, force=True)\n",
    "\n",
    "# Function to get ISO Alpha-2 code\n",
    "def get_country_code(country_name: str):\n",
    "    try:\n",
    "        # Faster but less robust pycountry matcher\n",
    "        country = pycountry.countries.lookup(country_name.strip())\n",
    "        return country.alpha_2\n",
    "    except LookupError:\n",
    "        # Fallback to coco which is more robust but slower\n",
    "        country = country_converter.convert(names=country_name, to='ISO2', not_found='NOT-FOUND')\n",
    "        return None if country == 'NOT-FOUND' else country\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply everything on the datasets and save the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted 12120 rows for year 2017\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/y5/_fnn1tf53cz1df_1wxhgm5m00000gn/T/ipykernel_44905/2029844218.py:34: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df[Column.AGE.value] = df[Column.AGE.value].fillna(df[Column.YEARS_OF_EXPERIENCE.value] + 23)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted 46467 rows for year 2018\n",
      "Extracted 55537 rows for year 2019\n",
      "Extracted 33333 rows for year 2020\n",
      "Extracted 46329 rows for year 2021\n",
      "Extracted 37891 rows for year 2022\n",
      "Extracted 47820 rows for year 2023\n",
      "Extracted 23309 rows for year 2024\n"
     ]
    }
   ],
   "source": [
    "save_path = Path(\"data/extracted\")\n",
    "os.makedirs(save_path, exist_ok=True)\n",
    "\n",
    "# Do not log warnings from coco (if match is not found)\n",
    "logging.basicConfig(level=logging.ERROR, force=True)\n",
    "\n",
    "dfs_normalized_by_year = {}\n",
    "\n",
    "for year, df in dfs_by_year.items():\n",
    "    if year not in colum_map_by_year:\n",
    "        continue\n",
    "\n",
    "    # Extract relevant columns\n",
    "    column_map = colum_map_by_year[year]\n",
    "    columns = [column_map[col] for col in Column if column_map[col] is not None]\n",
    "    df = df[columns]\n",
    "\n",
    "    # Rename columns\n",
    "    df.columns = [col.value for col in Column if column_map[col] is not None]\n",
    "\n",
    "    # Remove rows that don't have salary, language, country or years of experience\n",
    "    df = df.dropna(subset=[Column.SALARY.value, Column.PROGRAMMING_LANGUAGE.value, Column.COUNTRY.value, Column.YEARS_OF_EXPERIENCE.value])\n",
    "\n",
    "\n",
    "    # Year 2017 does not have age column, set values to None\n",
    "    if year == \"2017\":\n",
    "        df[Column.AGE.value] = None\n",
    "\n",
    "    # Convert y.o.e and age from range to numeric\n",
    "    df = convert_years_and_age(df)\n",
    "    print(f\"Extracted {len(df)} rows for year {year}\")\n",
    "\n",
    "    # Approximate age with years of experience + 23 if it's nan\n",
    "    df[Column.AGE.value] = df[Column.AGE.value].fillna(df[Column.YEARS_OF_EXPERIENCE.value] + 23)\n",
    "\n",
    "    # Standardize country names e.g. [USA, United States, ...] to country codes (US)\n",
    "    df[Column.COUNTRY_CODE.value] = df[Column.COUNTRY.value].apply(get_country_code)\n",
    "\n",
    "    # Remove rows with no country code (couldn't be matched, there's just 10-20 of them)\n",
    "    df = df.dropna(subset=[Column.COUNTRY_CODE.value])\n",
    "\n",
    "    df.to_csv(save_path / f\"{year}.csv\", index=False)\n",
    "    dfs_normalized_by_year[year] = df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extend over language column\n",
    "\n",
    "now the `language` column contains multiple programming languages concat'd by a delimiter e.g. \"Python; R; SQL\". We will split the rows into multiple rows, each containing a single language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "\n",
    "\n",
    "def parse_language(lang: str) -> Optional[str]:\n",
    "    lang = lang.strip()\n",
    "\n",
    "    if lang in [\"CSS\", \"SQL\", \"SQL Server\", \"MongoDB\", \"Node.js\"]:\n",
    "        # Let's be real, these are not real programming languages\n",
    "        return None\n",
    "\n",
    "    if \"bash\" in lang.lower() or \"html\" in lang.lower():\n",
    "        return None\n",
    "    \n",
    "    return lang\n",
    "\n",
    "def expand_by_language(df: pd.DataFrame):\n",
    "    # Create an empty list to store the expanded rows\n",
    "    expanded_rows = []\n",
    "\n",
    "    # Iterate over the rows of the dataframe\n",
    "    for _, row in df.iterrows():\n",
    "        # Split the 'language' column by ';'\n",
    "        languages = row[Column.PROGRAMMING_LANGUAGE.value].split(';')\n",
    "\n",
    "        # For each language, create a new row with the same values but different language\n",
    "        for language in languages:\n",
    "            language = parse_language(language)\n",
    "            if language is None:\n",
    "                continue\n",
    "\n",
    "            expanded_row = row.copy() \n",
    "            expanded_row[Column.PROGRAMMING_LANGUAGE.value] = language\n",
    "            expanded_rows.append(expanded_row)\n",
    "\n",
    "    # Convert the list of expanded rows back into a DataFrame\n",
    "    expanded_df = pd.DataFrame(expanded_rows)\n",
    "\n",
    "    return expanded_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017: 12107 rows expanded to 38152 rows\n",
      "2018: 46428 rows expanded to 180642 rows\n",
      "2019: 55477 rows expanded to 189266 rows\n",
      "2020: 33319 rows expanded to 113856 rows\n",
      "2021: 46311 rows expanded to 166636 rows\n",
      "2022: 37875 rows expanded to 137698 rows\n",
      "2023: 47803 rows expanded to 180633 rows\n",
      "2024: 23305 rows expanded to 90008 rows\n"
     ]
    }
   ],
   "source": [
    "save_path = Path(\"data/expanded\")\n",
    "os.makedirs(save_path, exist_ok=True)\n",
    "\n",
    "expanded_df_by_year = {}\n",
    "\n",
    "\n",
    "for year, df in dfs_normalized_by_year.items():\n",
    "    df_expanded = expand_by_language(df)\n",
    "\n",
    "    print(f\"{year}: {len(df)} rows expanded to {len(df_expanded)} rows\")\n",
    "\n",
    "\n",
    "    expanded_df_by_year[year] = df_expanded\n",
    "\n",
    "    df_expanded.to_csv(save_path / f\"{year}.csv\", index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subsets the data for top `n` languages\n",
    "\n",
    "- since there are many too many unique programming languages visualizing all of them would result in cluttered graphs\n",
    "- subset only the top `n` most popular languages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JavaScript: 206159\n",
      "Python: 126607\n",
      "Java: 103976\n",
      "TypeScript: 96040\n",
      "C#: 95326\n",
      "PHP: 68739\n",
      "C++: 57007\n",
      "C: 46979\n",
      "Go: 32587\n",
      "Ruby: 25967\n",
      "Kotlin: 22364\n",
      "PowerShell: 20826\n",
      "Rust: 18854\n",
      "Swift: 17960\n",
      "R: 14143\n",
      "VBA: 13787\n",
      "Objective-C: 12538\n",
      "Assembly: 12199\n",
      "Scala: 11301\n",
      "Dart: 10251\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Initialize a Counter to accumulate language occurrences across all years\n",
    "language_counter = Counter()\n",
    "\n",
    "for year, df in expanded_df_by_year.items():\n",
    "    # Count the occurrences of each language in the expanded dataframe\n",
    "    language_counts = df[Column.PROGRAMMING_LANGUAGE.value].value_counts()\n",
    "    # Update the Counter with the language counts as a dictionary\n",
    "    language_counter.update(language_counts.to_dict())\n",
    "\n",
    "# After processing all years, print the overall language counts sorted by occurrence\n",
    "sorted_language_counts = language_counter.most_common()\n",
    "\n",
    "n = 20\n",
    "top_n_languages = sorted_language_counts[:n]\n",
    "\n",
    "# Print the top n languages\n",
    "for language, count in top_n_languages:\n",
    "    print(f\"{language}: {count}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove all irrelevant rows from the dataframes (ones that are not related to top 20 programming languages by popularity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017: 38152 rows filtered to 34219 rows\n",
      "2018: 180642 rows filtered to 160821 rows\n",
      "2019: 189266 rows filtered to 180275 rows\n",
      "2020: 113856 rows filtered to 111638 rows\n",
      "2021: 166636 rows filtered to 155463 rows\n",
      "2022: 137698 rows filtered to 126205 rows\n",
      "2023: 180633 rows filtered to 161740 rows\n",
      "2024: 90008 rows filtered to 80691 rows\n",
      "Total rows: 1011052\n"
     ]
    }
   ],
   "source": [
    "\n",
    "save_path = Path(\"data/cleaned\")\n",
    "os.makedirs(save_path, exist_ok=True)\n",
    "\n",
    "top_languages_set = {lang for lang, _ in top_n_languages}\n",
    "\n",
    "df_clean_by_year = {}\n",
    "\n",
    "total_rows = 0\n",
    "\n",
    "for year, df in expanded_df_by_year.items():\n",
    "    # Filter out rows with languages that are not in the top n languages\n",
    "    df_clean = df[df[Column.PROGRAMMING_LANGUAGE.value].isin(top_languages_set)]\n",
    "\n",
    "    df_clean = df_clean.dropna()\n",
    "    print(f\"{year}: {len(df)} rows filtered to {len(df_clean)} rows\")\n",
    "    total_rows += len(df_clean)\n",
    "\n",
    "    df_clean['year'] = year\n",
    "    df_clean_by_year[year] = df_clean\n",
    "    df_clean.to_csv(save_path / f\"{year}.csv\", index=False)\n",
    "\n",
    "print(f\"Total rows: {total_rows}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter outliers by IQR\n",
    "\n",
    "Some of the records are very visible outliers (e.g. a few records of 10M+ salary in Venezuela while the average of the rest is 10k). We can remove them by filtering with IQR (interquartile range) which is a measure of the spread of the data. Data points that are more than 1.5 times the IQR away from the median are considered outliers.\n",
    "\n",
    "- remove outliers by salary\n",
    "- remove outliers by years of experience\n",
    "- remove outliers by age\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017: 34219 -> 30006 rows\n",
      "2018: 160821 -> 126298 rows\n",
      "2019: 180275 -> 150503 rows\n",
      "2020: 111638 -> 93323 rows\n",
      "2021: 155463 -> 126913 rows\n",
      "2022: 126205 -> 101068 rows\n",
      "2023: 161740 -> 138667 rows\n",
      "2024: 80691 -> 66682 rows\n"
     ]
    }
   ],
   "source": [
    "min_rows_for_group = 10\n",
    "\n",
    "def iqr_mask(series):\n",
    "    q1, q3 = series.quantile([0.25, 0.75])\n",
    "    iqr = q3 - q1\n",
    "    return series.between(q1 - 1.5*iqr, q3 + 1.5*iqr)\n",
    "\n",
    "for year, df in df_clean_by_year.items():\n",
    "    original_size = len(df)\n",
    "    grouping_cols = [Column.COUNTRY.value, Column.PROGRAMMING_LANGUAGE.value]\n",
    "\n",
    "    # Define a function to filter a single group\n",
    "    def filter_group(group):\n",
    "        # Apply filtering conditions on the group\n",
    "        filtered = group[\n",
    "            iqr_mask(group[Column.SALARY.value]) &\n",
    "            iqr_mask(group[Column.YEARS_OF_EXPERIENCE.value]) &\n",
    "            iqr_mask(group[Column.AGE.value]) &\n",
    "            group[Column.SALARY.value].between(501, 499_999)\n",
    "        ]\n",
    "        # Return the filtered group if it meets the row threshold\n",
    "        return filtered if len(filtered) >= min_rows_for_group else pd.DataFrame()\n",
    "\n",
    "    # Apply the filtering function to each group and retain the results\n",
    "    filtered_groups = [\n",
    "        filter_group(group) for _, group in df.groupby(grouping_cols) if not group.empty\n",
    "    ]\n",
    "\n",
    "    # Combine all the filtered groups back into a single DataFrame\n",
    "    df_clean = pd.concat(filtered_groups, axis=0).reset_index(drop=True) if filtered_groups else pd.DataFrame()\n",
    "\n",
    "    print(f\"{year}: {original_size} -> {len(df_clean)} rows\")\n",
    "    df_clean_by_year[year] = df_clean\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge all survey years into one dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save into src/assets/data.csv from where the frontend will load it\n",
    "\n",
    "save_path = Path(\"src\") / \"assets\"\n",
    "\n",
    "df_all = pd.concat(df_clean_by_year.values(), ignore_index=True)\n",
    "df_all.to_csv(save_path / \"data.csv\", index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
